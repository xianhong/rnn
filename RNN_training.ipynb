{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('embedding.npy')\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fable_text = \"\"\"\n",
    "long ago , the mice had a general council to consider what measures\n",
    "they could take to outwit their common enemy , the cat . some said\n",
    "this , and some said that but at last a young mouse got up and said\n",
    "he had a proposal to make , which he thought would meet the case . \n",
    "you will all agree , said he , that our chief danger consists in the\n",
    "sly and treacherous manner in which the enemy approaches us . now , \n",
    "if we could receive some signal of her approach , we could easily\n",
    "escape from her . i venture , therefore , to propose that a small\n",
    "bell be procured , and attached by a ribbon round the neck of the cat\n",
    ". by this means we should always know when she was about , and could\n",
    "easily retire while she was in the neighbourhood . this proposal met\n",
    "with general applause , until an old mouse got up and said that is\n",
    "all very well , but who is to bell the cat ? the mice looked at one\n",
    "another and nobody spoke . then the old mouse said it is easy to\n",
    "propose impossible remedies .\n",
    "\"\"\"\n",
    "# Replace the carriage return with space.\n",
    "fable_text = fable_text.replace('\\n','')\n",
    "\n",
    "#this function puts all the words in a single column vector within a numpy array\n",
    "\n",
    "def read_data(raw_text):\n",
    " content = raw_text\n",
    " content = content.split() #splits the text by spaces (default split character)\n",
    " content = np.array(content)\n",
    " content = np.reshape(content, [-1, ])\n",
    " return content\n",
    "\n",
    "training_data = read_data(fable_text)\n",
    "\n",
    "#Create dictionary and reverse dictionary with word ids\n",
    "\n",
    "def build_dictionaries(words):\n",
    "    count = collections.Counter(words).most_common() #creates list of word/count pairs;\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dictionaries(training_data)\n",
    "\n",
    "doc_vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate = 0.001\n",
    "n_input = 3 # this is the number of words that are read at a time\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tf.data.Dataset` to build pipeline\n",
    "`batch_size`: Scalar placeholder to accept mini-batch size.\n",
    "\n",
    "`data_vector`: 1D placeholder to accept a vector of integers representing series of training words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "batch_size =  tf.placeholder(tf.int64,[])\n",
    "data_vector = tf.placeholder(tf.int32,[None,])\n",
    "data_size = tf.size(data_vector,out_type=tf.int64)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                (tf.stack([data_vector[0:data_size-3],\n",
    "                                           data_vector[1:data_size-2],\n",
    "                                           data_vector[2:data_size-1]],\n",
    "                                          axis=1),\n",
    "                                   tf.reshape(data_vector[3:data_size],[-1,1]))\n",
    "                                    )\n",
    "\n",
    "dataset = dataset.shuffle(data_size).repeat().batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "current_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part of compute graph. \n",
    "`W`: keeps vocabulary embedding vectors. \n",
    "\n",
    "`x`: a variable to store a batch of input words (represented by integers). During inference, `x` is used as a feedable object to receive a vector of integers representing input words.\n",
    "\n",
    "`pred`: Predicted embedding vector(s) for batch of next word(s).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0, [None, n_input])\n",
    "weights = tf.Variable(tf.random_normal([n_hidden_2, embedding_dim]))\n",
    "biases =  tf.Variable(tf.random_normal([embedding_dim]))\n",
    "\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])\n",
    "    # `trainable` set to False if we want to fix `W` during training\n",
    "    W = tf.Variable(tf.zeros(tf.shape(embedding_placeholder)),trainable=False, name=\"W\")\n",
    "embed_init = W.assign(embedding_placeholder)\n",
    "  \n",
    "x = current_batch[0]\n",
    "embedded_chars = tf.nn.embedding_lookup(W,x)\n",
    "yy = tf.nn.embedding_lookup(W,current_batch[1])\n",
    "y = tf.reshape(yy,shape=[-1,embedding_dim])\n",
    "\n",
    "#cell = tf.keras.layers.LSTMCell(n_hidden_2)\n",
    "# Stack up 2 LSTM cells to appear as one cell\n",
    "cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.LSTMCell(n_hidden_1),\n",
    "                                        tf.keras.layers.LSTMCell(n_hidden_2)])\n",
    "output = tf.keras.layers.RNN(cell)(embedded_chars)\n",
    "\n",
    "pred = tf.matmul(output, weights) + biases\n",
    "\n",
    "# Create loss function and optimizer\n",
    "cost = tf.nn.l2_loss(pred-y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Put variables initializer at the end of graph construction.\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training (Run the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:{} 2595.2969\n",
      "Loss:{} 186.89722\n",
      "Loss:{} 124.54463\n",
      "Loss:{} 158.2266\n",
      "Loss:{} 115.27281\n",
      "Loss:{} 124.21072\n",
      "Loss:{} 130.2757\n",
      "Loss:{} 95.86549\n",
      "Loss:{} 92.815384\n",
      "Loss:{} 84.93054\n",
      "Finished Optimization\n"
     ]
    }
   ],
   "source": [
    "# Map words in training data set to internal representation of integers.\n",
    "training_data_int = np.asarray([dictionary[a] for a in list(training_data)],dtype=np.int32)\n",
    "\n",
    "# Run the graph\n",
    "step=0\n",
    "\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "training_iters =500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run([embed_init,iterator.initializer] ,\n",
    "         feed_dict={embedding_placeholder: embedding,\n",
    "                    batch_size:BATCH_SIZE,\n",
    "                    data_vector:training_data_int})\n",
    "\n",
    "while step < training_iters:\n",
    "    _,loss= sess.run([optimizer, cost])\n",
    "    if (step % 50 ==0): print(\"Loss:{}\",loss)\n",
    "    step +=1\n",
    " \n",
    "print(\"Finished Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save checkpoints (ex: graph & variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n"
     ]
    }
   ],
   "source": [
    "tf.add_to_collection('ops',x)\n",
    "tf.add_to_collection('ops',W)\n",
    "tf.add_to_collection('ops',pred)\n",
    "\n",
    "np.save('reverse_dictionary.npy', reverse_dictionary) \n",
    "np.save('dictionary.npy', dictionary) \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './RNN_model',global_step=step)\n",
    "# Write out graph so that it can be viewed in tensorboard\n",
    "writer = tf.summary.FileWriter('./', sess.graph)\n",
    "writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
