{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "embedding_dict = np.load('embedding_dict.npy').item()\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fable_text = \"\"\"\n",
    "long ago , the mice had a general council to consider what measures\n",
    "they could take to outwit their common enemy , the cat . some said\n",
    "this , and some said that but at last a young mouse got up and said\n",
    "he had a proposal to make , which he thought would meet the case . \n",
    "you will all agree , said he , that our chief danger consists in the\n",
    "sly and treacherous manner in which the enemy approaches us . now , \n",
    "if we could receive some signal of her approach , we could easily\n",
    "escape from her . i venture , therefore , to propose that a small\n",
    "bell be procured , and attached by a ribbon round the neck of the cat\n",
    ". by this means we should always know when she was about , and could\n",
    "easily retire while she was in the neighbourhood . this proposal met\n",
    "with general applause , until an old mouse got up and said that is\n",
    "all very well , but who is to bell the cat ? the mice looked at one\n",
    "another and nobody spoke . then the old mouse said it is easy to\n",
    "propose impossible remedies .\n",
    "\"\"\"\n",
    "# Replace the carriage return with space.\n",
    "fable_text = fable_text.replace('\\n','')\n",
    "\n",
    "#this function puts all the words in a single column vector within a numpy array\n",
    "\n",
    "def read_data(raw_text):\n",
    " content = raw_text\n",
    " content = content.split() #splits the text by spaces (default split character)\n",
    " content = np.array(content)\n",
    " content = np.reshape(content, [-1, ])\n",
    " return content\n",
    "\n",
    "training_data = read_data(fable_text)\n",
    "\n",
    "#Create dictionary and reverse dictionary with word ids\n",
    "\n",
    "def build_dictionaries(words):\n",
    "    count = collections.Counter(words).most_common() #creates list of word/count pairs;\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dictionaries(training_data)\n",
    "\n",
    "\n",
    "#Create embedding array\n",
    "\n",
    "doc_vocab_size = len(dictionary)\n",
    "\n",
    "\n",
    "embeddings_tmp=[]\n",
    "\n",
    "for i in range(doc_vocab_size):\n",
    "    item = reverse_dictionary[i]\n",
    "    if item in embedding_dict.keys():\n",
    "        embeddings_tmp.append(tf.constant(embedding_dict[item],dtype=tf.float32))\n",
    "        #embeddings_tmp.append(tf.Variable(embedding_dict[item],trainable=True,dtype=tf.float32))\n",
    "        #rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)\n",
    "        #embeddings_tmp.append(tf.Variable(rand_num,trainable=True,dtype=tf.float32))\n",
    "    else:\n",
    "        rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)\n",
    "        embeddings_tmp.append(tf.Variable(rand_num,trainable=True,dtype=tf.float32))\n",
    "\n",
    "# final embedding array corresponds to dictionary of words in the document\n",
    "W0 = tf.convert_to_tensor(embeddings_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate = 0.001\n",
    "n_input = 3 # this is the number of words that are read at a time\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tf.data.Dataset` to build pipeline\n",
    "`batch_size`: Scalar placeholder to accept mini-batch size.\n",
    "\n",
    "`data_vector`: 1D placeholder to accept a vector of integers representing series of training words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  tf.placeholder(tf.int64,[])\n",
    "data_vector = tf.placeholder(tf.int32,[None,])\n",
    "data_size = tf.size(data_vector,out_type=tf.int64)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                (tf.stack([data_vector[0:data_size-3],\n",
    "                                           data_vector[1:data_size-2],\n",
    "                                           data_vector[2:data_size-1]],\n",
    "                                          axis=1),\n",
    "                                   tf.reshape(data_vector[3:data_size],[-1,1]))\n",
    "                                    )\n",
    "\n",
    "dataset = dataset.shuffle(data_size).repeat().batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "current_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part of compute graph.\n",
    "\n",
    "`W`: keeps vocabulary embedding vectors.\n",
    "\n",
    "`x`: a variable to store a batch of input words (represented by integers). During inference, `x` is used as a feedable object to receive a vector of integers representing input words.\n",
    "\n",
    "`pred`: Predicted embedding vector(s) for batch of next word(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0, [None, n_input])\n",
    "weights = tf.Variable(tf.random_normal([n_hidden_2, embedding_dim]))\n",
    "biases =  tf.Variable(tf.random_normal([embedding_dim]))\n",
    "\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[doc_vocab_size, embedding_dim]), trainable=False)\n",
    "  \n",
    "    x = current_batch[0]\n",
    "    embedded_chars = tf.nn.embedding_lookup(W0,x)\n",
    "    yy = tf.nn.embedding_lookup(W0,current_batch[1])\n",
    "    y = tf.reshape(yy,shape=[-1,embedding_dim])\n",
    "\n",
    " \n",
    "# reshape input data\n",
    "x_unstack = tf.unstack(embedded_chars, n_input, 1)\n",
    "\n",
    "# create RNN cells\n",
    "rnn_cell = rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(n_hidden_1,name='basic_lstm_cell'),\n",
    "                             tf.nn.rnn_cell.LSTMCell(n_hidden_2,name='basic_lstm_cell')])\n",
    "\n",
    "outputs, states = rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)\n",
    "\n",
    "# capture only the last output\n",
    "pred = tf.matmul(outputs[-1], weights) + biases\n",
    "\n",
    "# Create loss function and optimizer\n",
    "#cost = tf.reduce_mean(tf.nn.l2_loss(pred-y))\n",
    "cost = tf.nn.l2_loss(pred-y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "save_W = W.assign(W0)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training (Run the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:{} 2968.834\n",
      "Loss:{} 135.11447\n",
      "Loss:{} 134.31493\n",
      "Loss:{} 165.87051\n",
      "Loss:{} 102.77092\n",
      "Loss:{} 110.6465\n",
      "Loss:{} 90.925064\n",
      "Loss:{} 77.81599\n",
      "Loss:{} 92.27479\n",
      "Loss:{} 60.00712\n",
      "Finished Optimization\n"
     ]
    }
   ],
   "source": [
    "# Map words in training data set to internal representation of integers.\n",
    "training_data_int = np.asarray([dictionary[a] for a in list(training_data)],dtype=np.int32)\n",
    "\n",
    "\n",
    "# Run the graph\n",
    "step=0\n",
    "\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "training_iters =500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run([iterator.initializer] ,\n",
    "         feed_dict={batch_size:BATCH_SIZE,\n",
    "                    data_vector:training_data_int})\n",
    "sess.run(W0)\n",
    "\n",
    "while step < training_iters:\n",
    "    _,loss= sess.run([optimizer, cost])\n",
    "    if (step % 50 ==0): print(\"Loss:{}\",loss)\n",
    "    step +=1\n",
    "sess.run(save_W)\n",
    "print(\"Finished Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save checkpoints (ex: graph & variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n"
     ]
    }
   ],
   "source": [
    "tf.add_to_collection('ops',x)\n",
    "tf.add_to_collection('ops',W)\n",
    "tf.add_to_collection('ops',pred)\n",
    "\n",
    "np.save('reverse_dictionary.npy', reverse_dictionary) \n",
    "np.save('dictionary.npy', dictionary) \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './RNN_model',global_step=step)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
