{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfilepath_glove = 'glove.6B.300d.txt'\\nglove_vocab = []\\nglove_embd=[]\\nembedding_dict = {}\\n\\nfile = open(filepath_glove,'r',encoding='UTF-8')\\nfor line in file.readlines():\\n    row = line.strip().split(' ')\\n    vocab_word = row[0]\\n    glove_vocab.append(vocab_word)\\n    embed_vector = [float(i) for i in row[1:]] # convert to list of float\\n    embedding_dict[vocab_word]=embed_vector\\nfile.close()\\n \\nprint('Loaded GLOVE')\\n\\nglove_vocab_size = len(glove_vocab)\\nembedding_dim = len(embed_vector)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load GLOVE vectors\n",
    "\"\"\"\n",
    "filepath_glove = 'glove.6B.300d.txt'\n",
    "glove_vocab = []\n",
    "glove_embd=[]\n",
    "embedding_dict = {}\n",
    "\n",
    "file = open(filepath_glove,'r',encoding='UTF-8')\n",
    "for line in file.readlines():\n",
    "    row = line.strip().split(' ')\n",
    "    vocab_word = row[0]\n",
    "    glove_vocab.append(vocab_word)\n",
    "    embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "    embedding_dict[vocab_word]=embed_vector\n",
    "file.close()\n",
    " \n",
    "print('Loaded GLOVE')\n",
    "\n",
    "glove_vocab_size = len(glove_vocab)\n",
    "embedding_dim = len(embed_vector)\n",
    "\"\"\"\n",
    "#np.save('embedding_dict.npy', embedding_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = np.load('embedding_dict.npy').item()\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fable_text = \"\"\"\n",
    "long ago , the mice had a general council to consider what measures\n",
    "they could take to outwit their common enemy , the cat . some said\n",
    "this , and some said that but at last a young mouse got up and said\n",
    "he had a proposal to make , which he thought would meet the case . \n",
    "you will all agree , said he , that our chief danger consists in the\n",
    "sly and treacherous manner in which the enemy approaches us . now , \n",
    "if we could receive some signal of her approach , we could easily\n",
    "escape from her . i venture , therefore , to propose that a small\n",
    "bell be procured , and attached by a ribbon round the neck of the cat\n",
    ". by this means we should always know when she was about , and could\n",
    "easily retire while she was in the neighbourhood . this proposal met\n",
    "with general applause , until an old mouse got up and said that is\n",
    "all very well , but who is to bell the cat ? the mice looked at one\n",
    "another and nobody spoke . then the old mouse said it is easy to\n",
    "propose impossible remedies .\n",
    "\"\"\"\n",
    "# Replace the carriage return with space.\n",
    "fable_text = fable_text.replace('\\n','')\n",
    "\n",
    "#this function puts all the words in a single column vector within a numpy array\n",
    "\n",
    "def read_data(raw_text):\n",
    " content = raw_text\n",
    " content = content.split() #splits the text by spaces (default split character)\n",
    " content = np.array(content)\n",
    " content = np.reshape(content, [-1, ])\n",
    " return content\n",
    "\n",
    "training_data = read_data(fable_text)\n",
    "\n",
    "#Create dictionary and reverse dictionary with word ids\n",
    "\n",
    "def build_dictionaries(words):\n",
    "    count = collections.Counter(words).most_common() #creates list of word/count pairs;\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dictionaries(training_data)\n",
    "\n",
    "\n",
    "#Create embedding array\n",
    "\n",
    "doc_vocab_size = len(dictionary)\n",
    "\n",
    "embeddings_tmp=[]\n",
    "\n",
    "for i in range(doc_vocab_size):\n",
    "    item = reverse_dictionary[i] \n",
    "    if item in embedding_dict.keys():\n",
    "        embeddings_tmp.append(embedding_dict[item])\n",
    "    else:\n",
    "        rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)\n",
    "        embeddings_tmp.append(rand_num)\n",
    "\n",
    "# final embedding array corresponds to dictionary of words in the document\n",
    "embedding = np.asarray(embeddings_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate = 0.001\n",
    "n_input = 3 # this is the number of words that are read at a time\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tf.data.Dataset` to build pipeline\n",
    "`batch_size`: Scalar placeholder to accept mini-batch size.\n",
    "\n",
    "`data_vector`: 1D placeholder to accept a vector of integers representing series of training words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  tf.placeholder(tf.int64,[])\n",
    "data_vector = tf.placeholder(tf.int32,[None,])\n",
    "data_size = tf.size(data_vector,out_type=tf.int64)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                (tf.stack([data_vector[0:data_size-3],\n",
    "                                           data_vector[1:data_size-2],\n",
    "                                           data_vector[2:data_size-1]],\n",
    "                                          axis=1),\n",
    "                                   tf.reshape(data_vector[3:data_size],[-1,1]))\n",
    "                                    )\n",
    "\n",
    "dataset = dataset.shuffle(data_size).repeat().batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "current_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part of compute graph. \n",
    "`W`: keeps vocabulary embedding vectors. \n",
    "\n",
    "`x`: a variable to store a batch of input words (represented by integers). During inference, `x` is used as a feedable object to receive a vector of integers representing input words.\n",
    "\n",
    "`pred`: Predicted embedding vector(s) for batch of next word(s).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0, [None, n_input])\n",
    "weights = tf.Variable(tf.random_normal([n_hidden_2, embedding_dim]))\n",
    "biases =  tf.Variable(tf.random_normal([embedding_dim]))\n",
    "\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])\n",
    "    W = tf.Variable(tf.zeros(tf.shape(embedding_placeholder)),trainable=True, name=\"W\")\n",
    "embed_init = W.assign(embedding_placeholder)\n",
    "  \n",
    "x = current_batch[0]\n",
    "embedded_chars = tf.nn.embedding_lookup(W,x)\n",
    "yy = tf.nn.embedding_lookup(W,current_batch[1])\n",
    "y = tf.reshape(yy,shape=[-1,embedding_dim])\n",
    "\n",
    " \n",
    "# reshape input data\n",
    "x_unstack = tf.unstack(embedded_chars, n_input, 1)\n",
    "\n",
    "# create RNN cells\n",
    "rnn_cell = rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(n_hidden_1,name='basic_lstm_cell'),\n",
    "                             tf.nn.rnn_cell.LSTMCell(n_hidden_2,name='basic_lstm_cell')])\n",
    "\n",
    "outputs, states = rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)\n",
    "\n",
    "# capture only the last output\n",
    "pred = tf.matmul(outputs[-1], weights) + biases\n",
    "\n",
    "# Create loss function and optimizer\n",
    "cost = tf.nn.l2_loss(pred-y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Put variables initializer at the end of graph construction.\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training (Run the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:{} 2714.549\n",
      "Loss:{} 191.67282\n",
      "Loss:{} 122.11395\n",
      "Loss:{} 110.8033\n",
      "Loss:{} 96.38237\n",
      "Loss:{} 94.06582\n",
      "Loss:{} 60.91414\n",
      "Loss:{} 65.84248\n",
      "Loss:{} 52.95076\n",
      "Loss:{} 39.645107\n",
      "Finished Optimization\n"
     ]
    }
   ],
   "source": [
    "# Map words in training data set to internal representation of integers.\n",
    "training_data_int = np.asarray([dictionary[a] for a in list(training_data)],dtype=np.int32)\n",
    "\n",
    "# Run the graph\n",
    "step=0\n",
    "\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "training_iters =500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run([embed_init,iterator.initializer] ,\n",
    "         feed_dict={embedding_placeholder: embedding,\n",
    "                    batch_size:BATCH_SIZE,\n",
    "                    data_vector:training_data_int})\n",
    "\n",
    "while step < training_iters:\n",
    "    _,loss= sess.run([optimizer, cost])\n",
    "    if (step % 50 ==0): print(\"Loss:{}\",loss)\n",
    "    step +=1\n",
    " \n",
    "print(\"Finished Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save checkpoints (ex: graph & variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "[None, 3] has type list, but expected one of: int, long, bool\n"
     ]
    }
   ],
   "source": [
    "tf.add_to_collection('ops',x)\n",
    "tf.add_to_collection('ops',W)\n",
    "tf.add_to_collection('ops',pred)\n",
    "\n",
    "np.save('reverse_dictionary.npy', reverse_dictionary) \n",
    "np.save('dictionary.npy', dictionary) \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './RNN_model',global_step=step)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
